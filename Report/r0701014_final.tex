\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage[dvinames,table,xcdraw]{xcolor}
\usepackage[compact,small]{titlesec}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{marginnote}
\usepackage[top=1.8cm, bottom=1.8cm, outer=1.8cm, inner=1.8cm, heightrounded, marginparwidth=2.5cm, marginparsep=0.5cm]{geometry}
\usepackage{enumitem}
\setlist{noitemsep,parsep=2pt}
\newcommand{\highlight}[1]{\textcolor{kuleuven}{#1}}
\usepackage{pythonhighlight}
\usepackage[hidelinks]{hyperref}
%\usepackage{cleveref}
\usepackage{graphicx}
\usepackage[nottoc]{tocbibind}
\usepackage{url}
\usepackage{pgfplots}
\usepackage{layouts}

\newcommand{\nextyear}{\advance\year by 1 \the\year\advance\year by -1}
\newcommand{\thisyear}{\the\year}
\newcommand{\deadlineGroup}{November 27, \thisyear{} at 16:00 CET}
\newcommand{\deadlineCode}{December 18, \thisyear{} at 16:00 CET}
\newcommand{\deadlineReport}{January 4, \nextyear{} at 16:00 CET}

\newcommand{\ReplaceMe}[1]{{\color{blue}#1}}
\newcommand{\RemoveMe}[1]{{\color{purple}#1}}

\setlength{\parskip}{5pt}

%opening
\title{Evolutionary Algorithms: Final report}
\author{Sander Prenen (r0701014)}

\begin{document}
\fontfamily{ppl}
\selectfont{}

\maketitle

%\section{\RemoveMe{Formal requirements}} \label{sec_this}

%\RemoveMe{The report is structured for fair and efficient grading of over 100 individual projects in the space of only a few days. Please respect the exact structure of this document. You are allowed to remove sections \ref{sec_this} and \ref{sec_other}. Brevity is the soul of wit: a good report will be \textbf{around $7$ pages} long. The hard limit is 10 pages. 

%It is recommended that you use this \LaTeX{} template, but you are allowed to reproduce it with the same structure in a WYSIWYG-editor. The purple text containing our evaluation criteria can be removed. You should replace the blue text with your discussion. \textbf{The questions we ask in blue are there to guide which topics to discuss}, rather than an exact list of questions that must be answered. Feel free to add more items to discuss.

%This report should be uploaded to Toledo by \deadlineReport. It must be in the \textbf{Portable Document Format} (pdf) and must be named \texttt{r0123456\_intermediate.pdf}, where r0123456 should be replaced with your student number.}

\section{Metadata}

\begin{itemize}
 \item \textbf{Group members during group phase:} Karel Everaert and Matthias Maeyens
 \item \textbf{Time spent on group phase:} 15 hours
 \item \textbf{Time spent on final code:} 79 hours (including hyperparameter search)
 \item \textbf{Time spent on final report:} 10 hours
\end{itemize}

\section{Modifications since the group phase}

%\RemoveMe{\textbf{Goal:} Based on this section, we will evaluate insofar as you are able to analyse common problems arising in the design and implementation of evolutionary algorithms and your ability to effectively solve them.}

\subsection{Main improvements}

%\ReplaceMe{List the main changes that you implemented since the group phase. You do not need to explain the employed techniques in detail; for this, you should refer to the appropriate subsection of section 4 of the report.}

\paragraph{Initialization of the population:}A greedy heuristic was used to initialize the population in order to start from a fitter population than the random population used in the group phase.

\paragraph{Schemes:} Different schemes were carried out for all the main operators in the evolutionary algorithm (selection, mutation, recombination, elimination).

\paragraph{Implementation of a local search operator:} A local search operator is implemented to improve the candidate solutions and speed up the convergence.

\paragraph{Ability to 'restart' the algorithm:} When the algorithm gets stuck in a minimum, it is able to restart itself with a new population and tries to converge to another (better) minimum.


\paragraph{Parallelization of the most expensive parts of the algorithm:} Computationally expensive parts of the algorithm like the local search operator have been parallelized by using the \pyth{multiprocessing} library.


\subsection{Issues resolved}
%\ReplaceMe{Recall the list of issues from the group phase. Describe how you solved these issues in the individual phase.}

\paragraph{Mutation:} The mutation operator used in the group phase had little effect on larger problem sizes. A number of different mutation operators were implemented to cope with this problem.

\paragraph{Elimination scheme:} The algorithm created in the group phase had the problem that the fittest candidate solution got mutated unintentionally, making divergence from the optimal tour possible. This issue was fixed by integrating an elitism elimination scheme to always keep the fittest individual in the next generation.

\paragraph{Diversity promotion:} The implementation in the group phase suffered from a lack of diversity. This caused the algorithm to converge to local optima. This problem was solved by adding a diversity promotion scheme to the evolutionary algorithm.

\paragraph{Recombination operator:} The recombination operator used in the group phase did not preserve the order of the nodes in the tour, but rather tried to preserve the position of the nodes. Several new recombination operators were created to deal with this problem.

\section{Final design of the evolutionary algorithm}  \label{sec:evolutionary algorithm}

%\RemoveMe{\textbf{Goal:} Based on this section, we will evaluate insofar as you are able to design and implement an advanced, effective evolutionary algorithm for solving a model problem.}

%\ReplaceMe{In this section, you should describe all components of your final evolutionary algorithm and how they fit together.}

\subsection{Representation}

%\ReplaceMe{How do you represent the candidate solutions? What is your motivation to choose this one? What other options did you consider? How did you implement this specifically in Python (e.g., a list, set, numpy array, etc)?}

An individual is represented as a 1D \pyth{numpy} array with the order of the visited nodes. A \pyth{numpy} array is used rather than a plain python list, because of the more efficient manner in which these arrays get stored in memory and the vectorized operations that are available in the \pyth{numpy} library. A population is represented as a two dimensional array (each row is an individual). This choice was made for the same reasons as mentioned above. Since duplicate individuals are allowed a set cannot be used. This type of collection is also unordered which would require more computations.

\subsection{Initialization} \label{sec:init}

%\ReplaceMe{How do you initialize the population? How did you determine the number of individuals? Did you implement advanced initialization mechanisms (local search operators, heuristic solutions)? If so, describe them. Do you believe your approach maintains sufficient diversity? How do you ensure that your population enrichment scheme does not immediately take over the population? Did you implement other initialization schemes that did not make it to the final version? Why did you discard them? How did you determine the population size?}

The population is initialized by using a greedy algorithm. The algorithm starts in a node and picks the nearest not visited node as the next node. The starting points are decided at random. This is a variation on all nearest neighbors (ANN) as proposed by Kaabi and Harrath \cite{ann}. This heuristic method creates less diversity than random initialization. Therefore a diversity promotion scheme is implemented as described in section \ref{sec:diversity}. The random initialization method is still available by setting the boolean value of \texttt{use\_random\_initialization} to true. This version explores more of the search space, but it will never reach the objective value of the heuristic solution for the bigger problems in the given time span.\\
When the algorithm gets trapped in a (local) optimum, it will restart itself by initializing a new random population. These individuals are created using the random nearest insertion heuristic \cite{rni} and can then recombine with the original individuals to escape the optimum. This heuristic uses a random sequence of nodes and inserts it into an existing sequence whilst keeping the total length minimal.

\subsection{Selection operators}

%\ReplaceMe{Which selection operators did you implement? If they are not from the slides, describe them. Can you motivate why you chose this one? Are there parameters that need to be chosen? Did you use an advanced scheme to vary these parameters throughout the iterations? Did you try other selection operators not included in the final version? Why did you discard them?}

The following selection operators were tested: k-tournament selection, linear rank-based selection and roulette wheel selection with geometric decaying selection pressure. Some of these selection operators depend on parameters chosen by the user. For k-tournament this parameter is k, the number of individuals to be randomly selected for the tournament. The roulette wheel selection depends on the starting selection pressure and the decay for this selection pressure. The linear rank based selection does not need any parameters to be chosen.\\
The final version of the evolutionary algorithm uses the roulette wheel selection with geometric decay. By doing a hyperparameter search, as described in section \ref{sec:hyperparameter}, the values of the starting selection pressure and the decay are both set to 0.999. This value was set when the entire algorithm was finished and was based on the number of iterations that were typically needed in order to reach convergence. The generation of individuals in this operator is not done using inverse transform sampling, as this would require $\mathcal{O}(n)$ time per sample. Instead the Vose's alias method is used as explained by Schwarz \cite{schwarz, vose}. This method is able to sample in constant time, but uses $\mathcal{O}(n)$ time to initialize. This initialization can be reused to select all the individuals during the current generation.\\
A fitness proportionate selection scheme was briefly introduced but discarded due to the problems mentioned in the slides \cite{slides}.

\subsection{Mutation operators} \label{sec:mutation}

%\ReplaceMe{Which mutation operators did you implement? If they are not from the slides, describe them. How do you choose among several mutation operators? Do you believe it will introduce sufficient randomness? Can that be controlled with parameters? Do you use self-adaptivity? Do you use any other advanced parameter control mechanisms (e.g., variable across iterations)? Did you try other mutation operators not included in the final version? Why did you discard them?}

The mutation operator that existed in the group phase, the sequence swap operator, was simplified to a simple swap operator that swaps two nodes at random. This operator had little effect on larger problem instances due to its proportionally small range. Therefore three different operators were introduced: reverse sequence mutation (RSM), partial shuffle mutation (PSM) and a hybrid of the two, called hybridizing PSM RSM mutation (HPRM)~\cite{hprm}.\\
RSM chooses two nodes in the individual at random and reverses the sequence between the two nodes. PSM is similar but randomly shuffles the sequence between the nodes, thus introducing more randomness. HPRM is a combination of the two where the sequence between the nodes is reversed element-wise and shuffled after each reversion.\\
All the operators discussed cannot be controlled by a parameter to tune the amount of randomness they introduce. The final evolutionary algorithm uses RSM because it proved the best in the numerical experiments. The operator is applied with a probability $\alpha$ to an individual, with $\alpha = 0.2 *\frac{ \textit{mean objective}}{\textit{mean objective } + \textit{ best objective}}$. This idea is based on the idea from Kaabi and Harrath \cite{ann}. 

\subsection{Recombination operators}

%\ReplaceMe{Which recombination operators did you implement? If they are not from the slides, describe them. How do you choose among several recombination operators? Why did you choose these ones specifically? Explain how you believe that these operators can produce offspring that combine the best features from their parents. How does your operator behave if there is little overlap between the parents? Can your recombination be controlled with parameters; what behavior do they change? Do you use self-adaptivity? Do you use any other advanced parameter control mechanisms (e.g., variable across iterations)? Did you try other recombination operators not included in the final version? Why did you discard them? Did you consider recombination with arity strictly greater than 2?}

The recombination operator implemented during the group phase, partially mapped crossover (PMX), was kept. Two other recombination operators called order crossover (OX) and sequential constructive crossover (SCX) proposed by Davis \cite{davis} and Ahmed \cite{ahmed} were implemented. All three operators start from the same principle, two crossover points are chosen at random in both parents. The nodes between these points are then copied to the offspring. How the rest of the nodes are copied differs between the operators. PMX tries to keep the positions of the nodes consistent by using cycles in the parents. OX copies the unused nodes from the parent of which the nodes between the crossover points have not been copied to the child. SCX searches in both parents to the next unused node and picks the one that is the nearest to the node previously inserted in the child. This operator thus only creates one child from two parents. Therefore double the number of recombinations are needed in order to generate the same number of offspring.\\
Since OX and SCX respect the relative order of nodes in the parents, these will probably perform better than PMX on the asymmetric TSP. The experiments performed in section \ref{sec:hyperparameter} also show this. The final algorithm uses OX since it had better results on bigger problem instances. This operator guarantees that if an edge between two nodes is present in both parents, it will definitely be copied to the offspring. If there is little overlap between the parents, then the operator will practically copy half of both parents to the children. Due to the complexity of recombination operators on permutations, operators with arity greater than two have not been considered. The operators themselves don't depend on any parameters. They just create the number of children specified in the \texttt{offspring\_size} variable.

\subsection{Elimination operators}

%\ReplaceMe{Which elimination operators did you implement? If they are not from the slides, describe them. Why did you select this one? Are there parameters that need to be chosen? Did you use an advanced scheme to vary these parameters throughout the iterations? Did you try other elimination operators not included in the final version? Why did you discard them?} 

The only elimination scheme implemented is $(\lambda + \mu)$-elimination, which was created in the group phase. Since it is not guaranteed that the offspring will be better than the previous generation, this is the best option. To make sure that the best objective value will not diverge, elitism is applied to the population after elimination. This is described in section \ref{sec:other}.\\
The selection operators k-tournament and roulette wheel selection were also tested as elimination schemes, but the results of these proved worse than the $(\lambda + \mu)$-elimination. This was probably the case due to the time limit, because the selection operators had a higher diversity. This diversity caused that the algorithm was not able to converge as quickly.

\subsection{Local search operators}

%\ReplaceMe{What local search operators did you implement? Describe them. Did they cause a significant improvement in the performance of your algorithm? Why (not)? Did you consider other local search operators that did not make the cut? Why did you discard them? Are there parameters that need to be determined in your operator? Do you use an advanced scheme to determine them (e.g., adaptive or self-adaptive)?}

Three different local search operators where considered. All three operators are based on a neighborhood structure. The first one is a 3-opt method where a tour is seen as three different arcs (A-B-C). If the alternative A-C-B is shorter then this tour is kept. The second operator is a 2-opt operator. This operator swaps two neighboring nodes in an individual if it is shorter than the other way around. The last operator reverses the individual as a whole if it is shorter. This operator was not implemented since it would not have a big effect because of the way the distance matrix is generated.\\
The final algorithm uses the 3-opt local search operator based on the one proposed by Kanellakis and Papadimitriou \cite{kanellakispapadimitriou} and improved by Bentley \cite{bentley} and Nagata and Soler \cite{nagatasoler}. Instead of splitting the tour in arcs in all possible places, a list of nearest neighbors is used for each node. Once a first splitting point is chosen, then the second point is limited by this list of nearest neighbors. This reduces the computations needed to perform the local search while still receiving a good improvement on the individual. To reduce the computations even further, the first splitting point is chosen at random. This makes that there is not always a better individual found using the operator, but if the individual makes it to the next generation then another random splitting point will be chosen. This approach proved experimentally better than doing a full 3-opt search on the individuals.\\
The only parameter that needs to be chosen is the number of nearest neighbors that is kept in the neighbor list. Nagata and Soler suggested to use 15. This is also the value used in this evolutionary algorithm. 

\subsection{Diversity promotion mechanisms} \label{sec:diversity}

%\ReplaceMe{Did you implement a diversity promotion scheme? If yes, which one? If no, why not? Describe the mechanism you implemented. In what sense does the mechanism improve the performance of your evolutionary algorithm? Are there parameters that need to be determined? Did you use an advanced scheme to determine them?}

Two different diversity promotion schemes have been implemented. Fitness sharing in the elimination phase and the removal of duplicate individuals after elimination as proposed by Herrera-Poyatos and Herrera \cite{herrera}.\\
Fitness sharing is implemented as described in the slides of this course \cite{slides}. The distance function used is the number of edges that are different between two individuals. The computation of the distance function is very expensive and is therefore executed in parallel. This scheme needs a maximum distance $\sigma$ that will be considered in order to apply the penalties.\\
The removal of duplicate individuals is based on the objective values. If the objective values are not equal then the individuals cannot be the same. This check is much easier than the distance computation in fitness sharing. If two or more individuals are the same, then one is kept and the others are replaced by a greedy randomized algorithm. The individual is initialized starting from a random node and the next chosen node is picked randomly from the set restricted candidates. These candidates are all nodes that are less than $\sigma$ percent longer than the greedy option.\\
The final algorithm uses the greedy diversification option because it performed better on the larger problem instances. Computing the distances for all individuals proved to be too costly. The parameter $\sigma$ used in the greedy diversification algorithm is set to 10 as suggested by the authors of the paper.

\subsection{Stopping criterion}

%\ReplaceMe{Which stopping criterion did you implement? Did you combine several criteria?}

During the development phase, a combination of two criteria was used. The first one was a limit on the number of generations (500 in this case) and the second one was the number of generations in which the best solutions had not changed. Some trail-and-error showed that 30 was a good value for this. The former criterion was obsolete for the larger problems due to the expensive local search operator. Since the elitism scheme was used in the final version of the algorithm, the stopping criterion has been dropped in order to fully use the given time span.

\subsection{The main loop}

%\ReplaceMe{Describe the main loop of your evolutionary algorithm using a clear picture (preferred) or high-level pseudocode. In what order do you apply the various operators? Why that order? If you are using several selection, mutation, recombination, elimination, and local search operators, describe how you choose among the possibilities. Are you selecting/eliminating all individuals in parallel, or one by one? With or without replacement?}

Before the main loop starts, some parameters that depend on the tour size are initialized. The offspring is generated in the \texttt{recombination} function. In there the individuals are selected with replacement in parallel. Afterwards all parents and children are supplied to the \texttt{mutation} function. This function applies mutation with a given probability $\alpha$ specified in section \ref{sec:mutation}. Then the local search operator is applied on half of these individuals. Elimination and diversification is then applied to the optimized population. The next part is 'restarting' the algorithm if there is a risk of settling in a local minimum. Finally the results of this generation are sent to the reporter and the loop is restarted. The main loop can be seen in Figure \ref{code:main}.

\begin{figure}
\inputpython{../r0701014.py}{258}{307}
\caption{The main loop of the evolutionary algorithm}
\label{code:main}
\end{figure}


\subsection{Parameter selection} \label{sec:hyperparameter}

%\ReplaceMe{For all of the parameters that are not automatically determined by adaptivity or self-adaptivity (as you have described above), describe how you determined them. Did you perform a hyperparameter search? How did you do this? How did you determine these parameters would be valid both for small and large problem instances?}

Tuning all parameters of an evolutionary algorithm is a huge problem on its own. Therefore not all possible combinations of the parameters and operators were tested. The tuning process was split in multiple parts in order to decrease the time needed. The first part of this process was deciding the mutation and recombination operators out of the possibilities described in section \ref{sec:evolutionary algorithm}. All the different combinations of mutation and recombination operators were tested 20 times on the smallest tour. The most promising mutation operators were RSM and HPRM, but there was no recombination operator that was significantly better. The remaining mutation operators and all recombination operators were then tested on \texttt{tour194.csv} in order to attempt to create a more significant difference between the recombination operators. The experiment showed that OX and SCX performed similar whilst PMX performed a fraction worse. For the final evolutionary algorithm OX was chosen, because it typically needed less generations than SCX to converge. Note that during this experiment it was assumed that all operators had similar performance with the same parameters. This is normally not the case.\\

Now that the operators are chosen, different experiments can be performed on the parameters of the evolutionary algorithm. If a value of a parameter was suggested by the authors of a paper, then this value was used and not changed or included in the hyperparameter search. The following parameters were selected to perform a hyperparameter search on: the population size $\lambda$, the offspring size $\mu$ and the percentage of individuals to perform local search on $l$. In Table \ref{table:hyperparameter} all the tested values for the different parameters can be found.\\
The hyperparameter search was performed on the smallest problem instance \texttt{tour29}. Each combination was tested eight times in order to reduce the random effect of the operators. Since the mean values were not relevant as indicator for the performance, the success rate was used. Where success is defined as finding the global minimum tour length. Obviously bigger population and offspring sizes show a higher success rate, as seen in Figure \ref{fig:hyperparameter}, but there ain't no such thing as a free lunch \cite{freelunch}. Therefore promising values that demand less calculations, like a population size around 25 and an offspring size around 50, are explored in a new hyperparameter search on a larger problem instance.\\

This larger problem instance is \texttt{tour194} and the parameters used are 16, 25 and 32 for the population size~$\lambda$ and 40, 50 and 60 for the offspring size $\mu$. The search was not able to significantly differentiate between the parameters used. Therefore the values of these parameters can be chosen by personal preference. For the rest of this paper the values $\lambda = 16$ and $\mu = 50$ are used.

\begin{figure}
\centering
\scalebox{.6}{\input{../nofreelunch.pgf}}
\caption{Number of successes for different parameter values on \texttt{tour29}.}
\label{fig:hyperparameter}
\end{figure}

\begin{table}[]
\centering
\caption{Parameter values for the hyperparameter search}
\label{table:hyperparameter}
\begin{tabular}{|l|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
Parameter & Values                       \\ \hline
$\lambda$ & 8, 16, 32, 100               \\ \hline
$\mu$     & 8, 16, 32, 50, 100      \\ \hline
$l$       & 0, 0.25, 0.5, 0.75, 1   \\ \hline
\end{tabular}
\end{table}

\subsection{Other considerations} \label{sec:other}

%\ReplaceMe{Did you consider other items not listed above, such as elitism, multiobjective optimization strategies (e.g., island model, pareto front approximation), a parallel implementation, or other interesting computational optimizations (e.g. using advanced algorithms or data structures)? You can describe them here or add additional subsections as needed.}

Some operators, like the local search operator and the fitness sharing elimination, are computationally expensive. But the results of these operators don't depend on each other. Therefore these operators can be executed in parallel. To accomplish this a \texttt{RawArray} from \texttt{multiprocessing} is used for the distance matrix. This is a one dimensional array that cannot be locked, but this is not needed because the parallel processes will not alter the matrix.\\

Sometimes during the execution of the algorithm, it occurs that it gets stuck in a local minimum and the diversity and local search operators are not able to get the individual out of it. To cope with this problem, the evolutionary algorithm 'restarts' itself by initializing a completely new population when the best individual has not been changed for a set number of iterations. This initialization is done with the random nearest insertion algorithm as described in section \ref{sec:init}. \\

Elitism is used to stop the algorithm from diverging from the optimal value. If the best individual after elimination is worse than the best individual of the previous generation, then this individual is inserted and the current worst individual is discarded. 


\section{Numerical experiments}

%\RemoveMe{\textbf{Goal:} Based on this section and our execution of your code, we will evaluate the performance (time, quality of solutions) of your implementation and your ability to interpret and explain the results on benchmark problems.}

\subsection{Metadata}

%\ReplaceMe{What parameters are there to choose in your evolutionary algorithm? Which fixed parameter values did you use for all experiments below? If some parameters are determined based on information from the problem instance (e.g., number of cities), also report their specific values for the problems below.

%Report the main characteristics of the computer system on which you ran your evolutionary algorithm. Include the processor or CPU (including the number of cores and clock speed), the amount of main memory, and the version of Python 3.}

All experiments conducted in this section are done using the same parameter values. The parameters that have a fixed value can be found in Table \ref{table:experiments} and the dynamic parameters take the values specified in section \ref{sec:evolutionary algorithm}. The operators used are: roulette wheel selection, OX for recombination, RSM for mutation, parallel 3-opt for local search and duplicate individual removal as diversity promotion scheme. All problem instances were run for five minutes, but the view of the graphs has been limited to 30 generations after the best objective value had been found. This is done to show the convergence more clearly.\\

All numeric experiments were performed on a 64-bit version of Ubuntu 20.04 with a quad-core Intel i5-7300U @ 2.60 GHz and 16 GB of memory. Python version 3.8.5 was used.

\begin{table}[h]
\begin{center}
\caption{Static parameters used in the numerical experiments}
\label{table:experiments}
\begin{tabular}{|l|c||l|c|}\hline
\rowcolor[HTML]{C0C0C0}
Parameter	& Value		& Parameter	& Value \\ \hline
Population size $\lambda$	&16 	&Offspring size $\mu$	&50 \\
Number of nearest neighbors	& 15	& Selection pressure	& 0.999 \\
Margin $\sigma$ in greedy diversification	&0.10	&Selection pressure decay	&0.999 \\
\hline
\end{tabular}
\end{center}
\end{table}


\subsection{tour29.csv}

%\ReplaceMe{Run your algorithm on this benchmark problem (with the 5 minute time limit from the Reporter). Include a typical convergence graph, by plotting the mean and best objective values in function of the time (for example based on the output of the Reporter class). 

%What is the best tour length you found? What is the corresponding sequence of cities? 

%Interpret your results. How do you rate the performance of your algorithm (time, memory, speed of convergence, diversity of population, quality of the best solution, etc)? Is your solution close to the optimal one?

%Solve this problem 1000 times and record the results. Make a histogram of the final mean fitnesses and the final best fitnesses of the 1000 runs. Comment on this figure: is there a lot of variability in the results, what are the means and the standard deviations?}

In Figure \ref{fig:graphs} the convergence graph of this problem can be found in the top left hand corner. Since the heuristic value is a good approximation for the optimal solution, it converges quickly. Because of these large values a plot with only the convergence of the best objective value has been added in Figure \ref{fig:best}.\\
The solution found is the global optimal solution for this instance. The corresponding sequence is:
$$14,11,10,9,5,0,1,4,7,3,2,6,8,12,13,15,23,24,26,19,25,27,28,22,21,20,16,17,18$$
It finds this solution very quickly. In Figure \ref{fig:best}, it can clearly be seen that the optimal solution is found around generation 25, after approximately 2 seconds. It is also very memory efficient since all the operations happen in place. In Figure \ref{fig:memory} the results of the memory profiling can be seen for the duration of the algorithm. The blue brackets mark the start and the end of the optimize function call. Since the results of the bigger tours show similar results, apart from the height of the plateau, these graphs will be omitted.\\
The algorithm will generate a different solution each time it is executed due to the random effects present. To study the variations in the solutions, the problem has been solved 1000 times. A histogram of the mean and best objective values is shown in Figure \ref{fig:histogram}. Note that the left-most bin of the best objective value extends up to 890, but the view of the plot has been restricted to 80 in order to make the other bins visible. The averages of the mean and best values are 29737 and 27206 respectively, while the standard deviations are 731 and 157.

\begin{figure}
\centering
\includegraphics[scale=0.7]{../memory_usage_crop.png}
\caption{Memory profiling of the \texttt{optimize} routine for \texttt{tour29}.}
\label{fig:memory}
\end{figure}

\begin{figure}
\centering
\scalebox{1}{\input{../histogram.pgf}}
\caption{Histogram of the mean and best objective values of 1000 runs of the \texttt{tour29} problem instance.}
\label{fig:histogram}
\end{figure}

\subsection{tour100.csv}

%\ReplaceMe{Run your algorithm on this benchmark problem (with the 5 minute time limit from the Reporter). Include a typical convergence graph, by plotting the mean and best objective values in function of the time (for example based on the output of the Reporter class). 
%
%What is the best tour length you found in each case? 
%
%Interpret your results. How do you rate the performance of your algorithm (time, memory, speed of convergence, diversity of population, quality of the best solution, etc)? Is your solution close to the optimal one?}

Just like the convergence graph of \texttt{tour29}, the convergence graph of \texttt{tour100} can be found on Figure \ref{fig:graphs} and a clearer view of the best value on is shown Figure \ref{fig:best}. Note that the mean objective value shows some incontinuities due to the disconnected cities. The algorithm does not explicitly take these disconnections into account, but rather treats it as an edge with an infinitely high distance. Therefore the recombination and mutation operators are able to create tours with an infinite length. This does not pose a problem for the best solution since the elimination and selection schemes will filter these results out.\\
The solution found is only 1\% longer than the optimal value specified in the project description and this optimal value is an estimation in itself. So the possibility exists that the solution found is even more optimal. The solution is also found well within the time limit (after around 30 seconds). 

\newpage
\subsection{tour194.csv}

%\ReplaceMe{Run your algorithm on this benchmark problem (with the 5 minute time limit from the Reporter). Include a typical convergence graph, by plotting the mean and best objective values in function of the time (for example based on the output of the Reporter class). 
%
%What is the best tour length you found? 
%
%Interpret your results. How do you rate the performance of your algorithm (time, memory, speed of convergence, diversity of population, quality of the best solution, etc)? Is your solution close to the optimal one?}

Just like the convergence graph of \texttt{tour29}, the convergence graph of \texttt{tour194} can be found on Figure \ref{fig:graphs} and a clearer view of the best value on is shown Figure \ref{fig:best}. The various restart points are clearly visible in the mean objective value. This time the algorithm ran for the full time span given, but the final solution was already found after approximately 100 seconds. The solution found here belongs in the top 20\% of all students on the leaderboard (at the time of writing). This shows that the solution found is a good solution for this problem.

\subsection{tour929.csv}

%\ReplaceMe{Run your algorithm on this benchmark problem (with the 5 minute time limit from the Reporter). Include a typical convergence graph, by plotting the mean and best objective values in function of the time (for example based on the output of the Reporter class). 
%
%What is the best tour length you found? 
%
%Interpret your results. How do you rate the performance of your algorithm (time, memory, speed of convergence, diversity of population, quality of the best solution, etc)? Is your solution close to the optimal one? 
%
%Did your algorithm converge before the time limit? How many iterations did you perform?}

Just like the convergence graph of \texttt{tour29}, the convergence graph of \texttt{tour929} can be found on Figure \ref{fig:graphs} and a clearer view of the best value is shown on Figure \ref{fig:best}. The final best solution had a length of 102637.963, but the algorithm had not yet converged after five minutes and 135 generations. This solution is 7\% longer than the optimal value specified in the project description. But this is again an estimation, thus the difference can be smaller as well. The best value this algorithm was able to produce, when no time limit was applied, was 100810 in 544 generations and 27 minutes.

\begin{figure}[p]
\centering
\rotatebox{270}{\input{../graphs.pgf}}
\caption{Convergence graphs of the different benchmark tours.}
\label{fig:graphs}
\end{figure}

\begin{figure}[p]
\centering
\rotatebox{270}{\input{../best.pgf}}
\caption{Convergence graphs of the best objective values of the different benchmark tours.}
\label{fig:best}
\end{figure}

\section{Critical reflection}

%\RemoveMe{\textbf{Goal:} Based on this section, we will evaluate your understanding and insight into the main strengths and weaknesses of your evolutionary algorithms.}

%\ReplaceMe{Describe the main lessons learned from this project. What do you think are the main strong points of evolutionary algorithms in general? Did you apply these strengths in this project? What are the main weaknesses of evolutionary algorithms and of your implementation in particular? Do you think these can be avoided or mitigated? How? Do you believe evolutionary algorithms are appropriate for this problem? Why (not)? What surprised you and why? What did you learn from this project?}

This project was very useful to get a better understanding of the workings of evolutionary algorithms. I knew that these algorithms existed and had seen them applied to games in different YouTube videos \cite{video1, video2}, but the fact that they can be applied to other problems as well was new to me. \\
I think that one of the key strengths of evolutionary algorithms is the fact that the search space can be explored in a structured way instead of sampling the domain at random and hoping for a good solution. But this strength can be its downfall as well. If the algorithm finds a local optimum, it can get stuck in this optimum. This weakness is further increased by the use of local search operators. Luckily diversion promotion schemes exist that try to counteract these weaknesses.\\
All these operators are used in the algorithm described in this paper. The biggest weakness of this algorithm is the local search operator. It is very powerful and this sometimes causes the algorithm to converge to local optima. Different diversity promotion schemes were tested to handle this problem but none of them were ideal. Some were computationally too expensive to perform in the five minute time span, whilst others were too disruptive. This caused that the algorithm essentially needed to converge twice in the five minutes given. A less powerful local search operator could mitigate the problem, but then more time is needed in order to converge. I think that the algorithm I have implemented is a good compromise between computation and results.\\
Another weakness of evolutionary algorithms is the need for parameter tuning.  This surprised me, because most of the times the results of the parameters 'out-of-the-box' are significantly worse than the tuned parameters. I did not know there were so many parameters to choose. Self-adaptivity can help with this problem, but sometimes these schemes require starting parameters themselves. In this case the parameter selection took almost 30 times longer than the algorithm is allowed to run. It would almost be worth using an evolutionary algorithm to set the parameters of the evolutionary algorithm.\\
I will definitely try out evolutionary algorithms on other problems in the future. The capabilities are almost endless with the different mutation, recombination and other operators. 

%\section{Other comments} \label{sec_other}

%\ReplaceMe{In case you think there is something important to discuss that is not covered by the previous sections, you can do it here. }
\newpage
\bibliographystyle{unsrt}
\bibliography{mybib}

\end{document}
